{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 38760,
     "databundleVersionId": 4493939,
     "sourceType": "competition"
    },
    {
     "sourceId": 4436180,
     "sourceType": "datasetVersion",
     "datasetId": 2597726
    }
   ],
   "dockerImageVersionId": 30302,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Candidate ReRank Model using Handcrafted Rules\nIn this notebook, we present a \"candidate rerank\" model using handcrafted rules. We can improve this model by engineering features, merging them unto items and users, and training a reranker model (such as XGB) to choose our final 20. Furthermore to tune and improve this notebook, we should build a local CV scheme to experiment new logic and/or models.\n\nUPDATE: I published a notebook to compute validation score [here][10] using Radek's scheme described [here][11].\n\nNote in this competition, a \"session\" actually means a unique \"user\". So our task is to predict what each of the `1,671,803` test \"users\" (i.e. \"sessions\") will do in the future. For each test \"user\" (i.e. \"session\") we must predict what they will `click`, `cart`, and `order` during the remainder of the week long test period.\n\n### Step 1 - Generate Candidates\nFor each test user, we generate possible choices, i.e. candidates. In this notebook, we generate candidates from 5 sources:\n* User history of clicks, carts, orders\n* Most popular 20 clicks, carts, orders during test week\n* Co-visitation matrix of click/cart/order to cart/order with type weighting\n* Co-visitation matrix of cart/order to cart/order called buy2buy\n* Co-visitation matrix of click/cart/order to clicks with time weighting\n\n### Step 2 - ReRank and Choose 20\nGiven the list of candidates, we must select 20 to be our predictions. In this notebook, we do this with a set of handcrafted rules. We can improve our predictions by training an XGBoost model to select for us. Our handcrafted rules give priority to:\n* Most recent previously visited items\n* Items previously visited multiple times\n* Items previously in cart or order\n* Co-visitation matrix of cart/order to cart/order\n* Current popular items\n\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Nov-2022/c_r_model.png)\n  \n# Credits\nWe thank many Kagglers who have shared ideas. We use co-visitation matrix idea from Vladimir [here][1]. We use groupby sort logic from Sinan in comment section [here][4]. We use duplicate prediction removal logic from Radek [here][5]. We use multiple visit logic from Pietro [here][2]. We use type weighting logic from Ingvaras [here][3]. We use leaky test data from my previous notebook [here][4]. And some ideas may have originated from Tawara [here][6] and KJ [here][7]. We use Colum2131's parquets [here][8]. Above image is from Ravi's discussion about candidate rerank models [here][9]\n\n[1]: https://www.kaggle.com/code/vslaykovsky/co-visitation-matrix\n[2]: https://www.kaggle.com/code/pietromaldini1/multiple-clicks-vs-latest-items\n[3]: https://www.kaggle.com/code/ingvarasgalinskas/item-type-vs-multiple-clicks-vs-latest-items\n[4]: https://www.kaggle.com/code/cdeotte/test-data-leak-lb-boost\n[5]: https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic\n[6]: https://www.kaggle.com/code/ttahara/otto-mors-aid-frequency-baseline\n[7]: https://www.kaggle.com/code/whitelily/co-occurrence-baseline\n[8]: https://www.kaggle.com/datasets/columbia2131/otto-chunk-data-inparquet-format\n[9]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364721\n[10]: https://www.kaggle.com/cdeotte/compute-validation-score-cv-564\n[11]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991",
   "metadata": {
    "papermill": {
     "duration": 0.005198,
     "end_time": "2022-11-10T16:03:20.966987",
     "exception": false,
     "start_time": "2022-11-10T16:03:20.961789",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": "# Notes\nBelow are notes about versions:\n* **Version 1 LB 0.573** Uses popular ideas from public notebooks and adds additional co-visitation matrices and additional logic. Has CV `0.563`. See validation notebook version 2 [here][1].\n* **Version 2 LB 573** Refactor logic for `suggest_buys(df)` to make it clear how new co-visitation matrices are reranking the candidates by adding to candidate weights. Also new logic boosts CV by `+0.0003`. Also LB is slightly better too. See validation notebook version 3 [here][1]\n* **Version 3** is the same as version 2 but 1.5x faster co-visitation matrix computation!\n* **Version 4 LB 575** Use top20 for clicks and top15 for carts and buys (instead of top40 and top40). This boosts CV `+0.0015` hooray! New CV is `0.5647`. See validation version 5 [here][1]\n* **Version 5** is the same as version 4 but 2x faster co-visitation matrix computation! (and 3x faster than version 1)\n* **Version 6** Stay tuned for more versions...\n\n[1]: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-564",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Step 1 - Candidate Generation with RAPIDS\nFor candidate generation, we build three co-visitation matrices. One computes the popularity of cart/order given a user's previous click/cart/order. We apply type weighting to this matrix. One computes the popularity of cart/order given a user's previous cart/order. We call this \"buy2buy\" matrix. One computes the popularity of clicks given a user previously click/cart/order.  We apply time weighting to this matrix. We will use RAPIDS cuDF GPU to compute these matrices quickly!",
   "metadata": {
    "papermill": {
     "duration": 0.00373,
     "end_time": "2022-11-10T16:03:20.9748",
     "exception": false,
     "start_time": "2022-11-10T16:03:20.97107",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from rich.jupyter import display\n",
    "\n",
    "VER = 5\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "\n",
    "print('We will use RAPIDS version', cudf.__version__)\n"
   ],
   "metadata": {
    "papermill": {
     "duration": 3.036143,
     "end_time": "2022-11-10T16:03:24.014816",
     "exception": false,
     "start_time": "2022-11-10T16:03:20.978673",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-10-21T00:20:04.109033Z",
     "iopub.execute_input": "2024-10-21T00:20:04.110108Z",
     "iopub.status.idle": "2024-10-21T00:20:06.606464Z",
     "shell.execute_reply.started": "2024-10-21T00:20:04.110013Z",
     "shell.execute_reply": "2024-10-21T00:20:06.605349Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T10:36:37.842624Z",
     "start_time": "2024-10-21T10:36:37.834147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use RAPIDS version 24.10.01\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": "## Compute Three Co-visitation Matrices with RAPIDS\nWe will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable `DISK_PIECES` to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation\n* Use RAPIDS cuDF GPU instead of Pandas CPU\n* Read disk once and save in CPU RAM for later GPU multiple use\n* Process largest amount of data possible on GPU at one time\n* Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n* Write result as parquet instead of dictionary",
   "metadata": {
    "papermill": {
     "duration": 0.00424,
     "end_time": "2022-11-10T16:03:24.023816",
     "exception": false,
     "start_time": "2022-11-10T16:03:24.019576",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "# CACHE FUNCTIONS\n",
    "def read_file(f):\n",
    "    return cudf.DataFrame(data_cache[f])\n",
    "\n",
    "\n",
    "def read_file_to_cache(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts / 1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df\n",
    "\n",
    "\n",
    "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
    "data_cache = {}\n",
    "type_labels = {'clicks': 0, 'carts': 1, 'orders': 2}\n",
    "files = glob.glob('./parquet/*_parquet/*')\n",
    "for f in files: data_cache[f] = read_file_to_cache(f)\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 3\n",
    "CHUNK = int(np.ceil(len(files) / 6))\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ],
   "metadata": {
    "papermill": {
     "duration": 0.063943,
     "end_time": "2022-11-10T16:03:24.091816",
     "exception": false,
     "start_time": "2022-11-10T16:03:24.027873",
     "status": "completed"
    },
    "tags": [],
    "_kg_hide-input": true,
    "execution": {
     "iopub.status.busy": "2024-10-21T00:20:06.608415Z",
     "iopub.execute_input": "2024-10-21T00:20:06.608756Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T10:36:47.539209Z",
     "start_time": "2024-10-21T10:36:39.387560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will process 116 files, in groups of 3 and chunks of 20.\n",
      "CPU times: user 11.5 s, sys: 3.11 s, total: 14.6 s\n",
      "Wall time: 8.15 s\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": "## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted",
   "metadata": {
    "papermill": {
     "duration": 0.004089,
     "end_time": "2022-11-10T16:03:24.100502",
     "exception": false,
     "start_time": "2022-11-10T16:03:24.096413",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "type_weight = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 20\n",
    "SIZE = 1.86e6 / DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART', PART + 1)\n",
    "\n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j * CHUNK\n",
    "        b = min((j + 1) * CHUNK, len(files))\n",
    "        print(f'Processing files {a} thru {b - 1} in groups of {READ_CT}...')\n",
    "\n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a, b, READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1, READ_CT):\n",
    "                if k + i < b: df.append(read_file(files[k + i]))\n",
    "            df = cudf.concat(df, ignore_index=True, axis=0)\n",
    "            df = df.sort_values(['session', 'ts'], ascending=[True, False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n < 30].drop('n', axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df, on='session')\n",
    "            df = df.loc[((df.ts_x - df.ts_y).abs() < 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART * SIZE) & (df.aid_x < (PART + 1) * SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y', 'type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = df.type_y.map(type_weight)\n",
    "            df = df[['aid_x', 'aid_y', 'wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x', 'aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k == a:\n",
    "                tmp2 = df\n",
    "            else:\n",
    "                tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k, ', ', end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a == 0:\n",
    "            tmp = tmp2\n",
    "        else:\n",
    "            tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x', 'wgt'], ascending=[True, False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n < 15].drop('n', axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt')"
   ],
   "metadata": {
    "papermill": {
     "duration": 566.561189,
     "end_time": "2022-11-10T16:12:50.666123",
     "exception": false,
     "start_time": "2022-11-10T16:03:24.104934",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T10:36:52.997156Z",
     "start_time": "2024-10-21T10:36:52.934704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 19 in groups of 3...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc: out_of_memory: CUDA error at: /home/chenzhen/miniconda3/envs/rapids/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorMemoryAllocation out of memory",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "File \u001B[0;32m<timed exec>:32\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/rapids/lib/python3.12/site-packages/cudf/utils/performance_tracking.py:51\u001B[0m, in \u001B[0;36m_performance_tracking.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m nvtx\u001B[38;5;241m.\u001B[39menabled():\n\u001B[1;32m     44\u001B[0m     stack\u001B[38;5;241m.\u001B[39menter_context(\n\u001B[1;32m     45\u001B[0m         nvtx\u001B[38;5;241m.\u001B[39mannotate(\n\u001B[1;32m     46\u001B[0m             message\u001B[38;5;241m=\u001B[39mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m         )\n\u001B[1;32m     50\u001B[0m     )\n\u001B[0;32m---> 51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/envs/rapids/lib/python3.12/site-packages/cudf/core/dataframe.py:4299\u001B[0m, in \u001B[0;36mDataFrame.merge\u001B[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001B[0m\n\u001B[1;32m   4284\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m how \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleftsemi\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleftanti\u001B[39m\u001B[38;5;124m\"\u001B[39m}:\n\u001B[1;32m   4285\u001B[0m     merge_cls \u001B[38;5;241m=\u001B[39m MergeSemi\n\u001B[1;32m   4287\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m merge_cls(\n\u001B[1;32m   4288\u001B[0m     lhs,\n\u001B[1;32m   4289\u001B[0m     rhs,\n\u001B[1;32m   4290\u001B[0m     on\u001B[38;5;241m=\u001B[39mon,\n\u001B[1;32m   4291\u001B[0m     left_on\u001B[38;5;241m=\u001B[39mleft_on,\n\u001B[1;32m   4292\u001B[0m     right_on\u001B[38;5;241m=\u001B[39mright_on,\n\u001B[1;32m   4293\u001B[0m     left_index\u001B[38;5;241m=\u001B[39mleft_index,\n\u001B[1;32m   4294\u001B[0m     right_index\u001B[38;5;241m=\u001B[39mright_index,\n\u001B[1;32m   4295\u001B[0m     how\u001B[38;5;241m=\u001B[39mhow,\n\u001B[1;32m   4296\u001B[0m     sort\u001B[38;5;241m=\u001B[39msort,\n\u001B[1;32m   4297\u001B[0m     indicator\u001B[38;5;241m=\u001B[39mindicator,\n\u001B[1;32m   4298\u001B[0m     suffixes\u001B[38;5;241m=\u001B[39msuffixes,\n\u001B[0;32m-> 4299\u001B[0m )\u001B[38;5;241m.\u001B[39mperform_merge()\n",
      "File \u001B[0;32m~/miniconda3/envs/rapids/lib/python3.12/site-packages/cudf/core/join/join.py:289\u001B[0m, in \u001B[0;36mMerge.perform_merge\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    282\u001B[0m left_rows, right_rows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gather_maps(\n\u001B[1;32m    283\u001B[0m     left_join_cols, right_join_cols\n\u001B[1;32m    284\u001B[0m )\n\u001B[1;32m    285\u001B[0m gather_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeep_index\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_using_left_index \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_using_right_index,\n\u001B[1;32m    287\u001B[0m }\n\u001B[1;32m    288\u001B[0m left_result \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 289\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlhs\u001B[38;5;241m.\u001B[39m_gather(\n\u001B[1;32m    290\u001B[0m         GatherMap\u001B[38;5;241m.\u001B[39mfrom_column_unchecked(\n\u001B[1;32m    291\u001B[0m             left_rows, \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlhs), nullify\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    292\u001B[0m         ),\n\u001B[1;32m    293\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mgather_kwargs,\n\u001B[1;32m    294\u001B[0m     )\n\u001B[1;32m    295\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m left_rows \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m cudf\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39m_from_data({})\n\u001B[1;32m    297\u001B[0m )\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m left_rows\n\u001B[1;32m    299\u001B[0m right_result \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrhs\u001B[38;5;241m.\u001B[39m_gather(\n\u001B[1;32m    301\u001B[0m         GatherMap\u001B[38;5;241m.\u001B[39mfrom_column_unchecked(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m cudf\u001B[38;5;241m.\u001B[39mDataFrame\u001B[38;5;241m.\u001B[39m_from_data({})\n\u001B[1;32m    308\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/rapids/lib/python3.12/site-packages/cudf/core/indexed_frame.py:2921\u001B[0m, in \u001B[0;36mIndexedFrame._gather\u001B[0;34m(self, gather_map, keep_index)\u001B[0m\n\u001B[1;32m   2918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m gather_map\u001B[38;5;241m.\u001B[39mnullify \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m!=\u001B[39m gather_map\u001B[38;5;241m.\u001B[39mnrows:\n\u001B[1;32m   2919\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGather map is out of bounds\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2920\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_from_columns_like_self(\n\u001B[0;32m-> 2921\u001B[0m     libcudf\u001B[38;5;241m.\u001B[39mcopying\u001B[38;5;241m.\u001B[39mgather(\n\u001B[1;32m   2922\u001B[0m         \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39m_columns \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_columns)\n\u001B[1;32m   2923\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m keep_index\n\u001B[1;32m   2924\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_columns),\n\u001B[1;32m   2925\u001B[0m         gather_map\u001B[38;5;241m.\u001B[39mcolumn,\n\u001B[1;32m   2926\u001B[0m         nullify\u001B[38;5;241m=\u001B[39mgather_map\u001B[38;5;241m.\u001B[39mnullify,\n\u001B[1;32m   2927\u001B[0m     ),\n\u001B[1;32m   2928\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_column_names,\n\u001B[1;32m   2929\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39mnames \u001B[38;5;28;01mif\u001B[39;00m keep_index \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   2930\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/rapids/lib/python3.12/contextlib.py:81\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[0;32m---> 81\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[0;32mcopying.pyx:153\u001B[0m, in \u001B[0;36mcudf._lib.copying.gather\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mcopying.pyx:39\u001B[0m, in \u001B[0;36mpylibcudf.copying.gather\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mcopying.pyx:71\u001B[0m, in \u001B[0;36mpylibcudf.copying.gather\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mMemoryError\u001B[0m: std::bad_alloc: out_of_memory: CUDA error at: /home/chenzhen/miniconda3/envs/rapids/include/rmm/mr/device/cuda_memory_resource.hpp:62: cudaErrorMemoryAllocation out of memory"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": "## 2) \"Buy2Buy\" Co-visitation Matrix",
   "metadata": {
    "papermill": {
     "duration": 0.03219,
     "end_time": "2022-11-10T16:12:50.730634",
     "exception": false,
     "start_time": "2022-11-10T16:12:50.698444",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 4\n",
    "SIZE = 1.86e6 / DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART', PART + 1)\n",
    "\n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j * CHUNK\n",
    "        b = min((j + 1) * CHUNK, len(files))\n",
    "        print(f'Processing files {a} thru {b - 1} in groups of {READ_CT}...')\n",
    "\n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a, b, READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1, READ_CT):\n",
    "                if k + i < b: df.append(read_file(files[k + i]))\n",
    "            df = cudf.concat(df, ignore_index=True, axis=0)\n",
    "            df = df.loc[df['type'].isin([1, 2])]  # ONLY WANT CARTS AND ORDERS\n",
    "            df = df.sort_values(['session', 'ts'], ascending=[True, False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n < 30].drop('n', axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df, on='session')\n",
    "            df = df.loc[((df.ts_x - df.ts_y).abs() < 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y)]  # 14 DAYS\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART * SIZE) & (df.aid_x < (PART + 1) * SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y', 'type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1\n",
    "            df = df[['aid_x', 'aid_y', 'wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x', 'aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k == a:\n",
    "                tmp2 = df\n",
    "            else:\n",
    "                tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k, ', ', end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a == 0:\n",
    "            tmp = tmp2\n",
    "        else:\n",
    "            tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x', 'wgt'], ascending=[True, False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n < 15].drop('n', axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "papermill": {
     "duration": 113.735315,
     "end_time": "2022-11-10T16:14:44.498182",
     "exception": false,
     "start_time": "2022-11-10T16:12:50.762867",
     "status": "completed"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T04:16:40.480867Z",
     "start_time": "2024-10-21T04:14:51.300265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "CPU times: user 1min 10s, sys: 37.7 s, total: 1min 48s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "execution_count": 165
  },
  {
   "cell_type": "markdown",
   "source": "## 3) \"Clicks\" Co-visitation Matrix - Time Weighted",
   "metadata": {
    "papermill": {
     "duration": 0.04526,
     "end_time": "2022-11-10T16:14:44.58589",
     "exception": false,
     "start_time": "2022-11-10T16:14:44.54063",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 20\n",
    "SIZE = 1.86e6 / DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART', PART + 1)\n",
    "\n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j * CHUNK\n",
    "        b = min((j + 1) * CHUNK, len(files))\n",
    "        print(f'Processing files {a} thru {b - 1} in groups of {READ_CT}...')\n",
    "\n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a, b, READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1, READ_CT):\n",
    "                if k + i < b: df.append(read_file(files[k + i]))\n",
    "            df = cudf.concat(df, ignore_index=True, axis=0)\n",
    "            df = df.sort_values(['session', 'ts'], ascending=[True, False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n < 30].drop('n', axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df, on='session')\n",
    "            df = df.loc[((df.ts_x - df.ts_y).abs() < 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART * SIZE) & (df.aid_x < (PART + 1) * SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y', 'ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1 + 3 * (df.ts_x - 1659304800) / (1662328791 - 1659304800)\n",
    "            df = df[['aid_x', 'aid_y', 'wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x', 'aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k == a:\n",
    "                tmp2 = df\n",
    "            else:\n",
    "                tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k, ', ', end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a == 0:\n",
    "            tmp = tmp2\n",
    "        else:\n",
    "            tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x', 'wgt'], ascending=[True, False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n < 20].drop('n', axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')"
   ],
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2022-11-10T16:14:44.629032",
     "status": "running"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T04:28:31.192230Z",
     "start_time": "2024-10-21T04:16:40.491325Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 4\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 5\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 6\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 7\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 8\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 9\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 10\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 11\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 12\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 13\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 14\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 15\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 16\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 17\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 18\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 19\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "\n",
      "### DISK PART 20\n",
      "Processing files 0 thru 19 in groups of 3...\n",
      "0 , 3 , 6 , 9 , 12 , 15 , 18 , \n",
      "Processing files 20 thru 39 in groups of 3...\n",
      "20 , 23 , 26 , 29 , 32 , 35 , 38 , \n",
      "Processing files 40 thru 59 in groups of 3...\n",
      "40 , 43 , 46 , 49 , 52 , 55 , 58 , \n",
      "Processing files 60 thru 79 in groups of 3...\n",
      "60 , 63 , 66 , 69 , 72 , 75 , 78 , \n",
      "Processing files 80 thru 99 in groups of 3...\n",
      "80 , 83 , 86 , 89 , 92 , 95 , 98 , \n",
      "Processing files 100 thru 115 in groups of 3...\n",
      "100 , 103 , 106 , 109 , 112 , 115 , \n",
      "CPU times: user 7min 30s, sys: 4min 18s, total: 11min 49s\n",
      "Wall time: 11min 50s\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "cell_type": "code",
   "source": "# FREE MEMORY\ndel data_cache, tmp\n_ = gc.collect()",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T04:28:31.302571Z",
     "start_time": "2024-10-21T04:28:31.205511Z"
    }
   },
   "outputs": [],
   "execution_count": 167
  },
  {
   "cell_type": "markdown",
   "source": "# Step 2 - ReRank (choose 20) using handcrafted rules\nFor description of the handcrafted rules, read this notebook's intro.",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def load_test():\n",
    "    dfs = []\n",
    "    for e, chunk_file in enumerate(glob.glob('./parquet/test_parquet/*')):\n",
    "        chunk = pd.read_parquet(chunk_file)\n",
    "        chunk.ts = (chunk.ts / 1000).astype('int32')\n",
    "        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n",
    "        dfs.append(chunk)\n",
    "    return pd.concat(dfs).reset_index(drop=True)  #.astype({\"ts\": \"datetime64[ms]\"})\n",
    "\n",
    "\n",
    "test_df = load_test()\n",
    "print('Test data has shape', test_df.shape)\n",
    "test_df.head()"
   ],
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T04:28:31.696315Z",
     "start_time": "2024-10-21T04:28:31.307106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data has shape (6928123, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    session      aid          ts  type\n",
       "0  12899779    59625  1661724000     0\n",
       "1  12899780  1142000  1661724000     0\n",
       "2  12899780   582732  1661724058     0\n",
       "3  12899780   973453  1661724109     0\n",
       "4  12899780   736515  1661724136     0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aid</th>\n",
       "      <th>ts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12899779</td>\n",
       "      <td>59625</td>\n",
       "      <td>1661724000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12899780</td>\n",
       "      <td>1142000</td>\n",
       "      <td>1661724000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12899780</td>\n",
       "      <td>582732</td>\n",
       "      <td>1661724058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12899780</td>\n",
       "      <td>973453</td>\n",
       "      <td>1661724109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12899780</td>\n",
       "      <td>736515</td>\n",
       "      <td>1661724136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 168
  },
  {
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T04:28:59.757513Z",
     "start_time": "2024-10-21T04:28:31.716546Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are size of our 3 co-visitation matrices:\n",
      "1820555 278986 1820555\n",
      "CPU times: user 28 s, sys: 2.3 s, total: 30.3 s\n",
      "Wall time: 28 s\n"
     ]
    }
   ],
   "execution_count": 169,
   "source": [
    "%%time\n",
    "\n",
    "def pqt_to_dict(df):\n",
    "    return df.groupby('aid_x').aid_y.apply(list).to_dict()\n",
    "\n",
    "\n",
    "# LOAD THREE CO-VISITATION MATRICES\n",
    "top_20_clicks = pqt_to_dict(pd.read_parquet(f'top_20_clicks_v{VER}_0.pqt'))\n",
    "for k in range(1, DISK_PIECES):\n",
    "    top_20_clicks.update(pqt_to_dict(pd.read_parquet(f'top_20_clicks_v{VER}_{k}.pqt')))\n",
    "top_20_buys = pqt_to_dict(pd.read_parquet(f'top_15_carts_orders_v{VER}_0.pqt'))\n",
    "for k in range(1, DISK_PIECES):\n",
    "    top_20_buys.update(pqt_to_dict(pd.read_parquet(f'top_15_carts_orders_v{VER}_{k}.pqt')))\n",
    "top_20_buy2buy = pqt_to_dict(pd.read_parquet(f'top_15_buy2buy_v{VER}_0.pqt'))\n",
    "\n",
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "top_clicks = test_df.loc[test_df['type'] == 'clicks', 'aid'].value_counts().index.values[:20]\n",
    "top_orders = test_df.loc[test_df['type'] == 'orders', 'aid'].value_counts().index.values[:20]\n",
    "\n",
    "print('Here are size of our 3 co-visitation matrices:')\n",
    "print(len(top_20_clicks), len(top_20_buy2buy), len(top_20_buys))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#type_weight_multipliers = {'clicks': 1, 'carts': 6, 'orders': 3}\n",
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "\n",
    "def suggest_clicks(df):\n",
    "    # USER HISTORY AIDS AND TYPES\n",
    "    aids = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights = np.logspace(0.1, 1, len(aids), base=2, endpoint=True) - 1\n",
    "        aids_temp = Counter()\n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid, w, t in zip(aids, weights, types):\n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        sorted_aids = [k for k, v in aids_temp.most_common(20)]\n",
    "        return sorted_aids\n",
    "    # USE \"CLICKS\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]\n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    # USE TOP20 TEST CLICKS\n",
    "    return result + list(top_clicks)[:20 - len(result)]\n",
    "\n",
    "\n",
    "def suggest_buys(df):\n",
    "    # USER HISTORY AIDS AND TYPES\n",
    "    aids = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1]))\n",
    "    df = df.loc[(df['type'] == 1) | (df['type'] == 2)]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids) >= 20:\n",
    "        weights = np.logspace(0.5, 1, len(aids), base=2, endpoint=True) - 1\n",
    "        aids_temp = Counter()\n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid, w, t in zip(aids, weights, types):\n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "        for aid in aids3: aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k, v in aids_temp.most_common(20)]\n",
    "        return sorted_aids\n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2 + aids3).most_common(20) if aid2 not in unique_aids]\n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    # USE TOP20 TEST ORDERS\n",
    "    return result + list(top_orders)[:20 - len(result)]"
   ],
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T04:28:59.821722Z",
     "start_time": "2024-10-21T04:28:59.817773Z"
    }
   },
   "outputs": [],
   "execution_count": 170
  },
  {
   "cell_type": "markdown",
   "source": "# Create Submission CSV\nInferring test data with Pandas groupby is slow. We need to accelerate the following code.",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": "%%time\npred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n    lambda x: suggest_clicks(x)\n)\n\npred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n    lambda x: suggest_buys(x)\n)",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T04:34:49.360996Z",
     "start_time": "2024-10-21T04:28:59.834724Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 49s, sys: 2.9 s, total: 5min 52s\n",
      "Wall time: 5min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n"
     ]
    }
   ],
   "execution_count": 171
  },
  {
   "cell_type": "code",
   "source": "clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\norders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\ncarts_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T04:34:50.142048Z",
     "start_time": "2024-10-21T04:34:49.434949Z"
    }
   },
   "outputs": [],
   "execution_count": 172
  },
  {
   "cell_type": "code",
   "source": [
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
    "pred_df.columns = [\"session_type\", \"labels\"]\n",
    "pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str, x)))\n",
    "pred_df.to_csv(\"submission.csv\", index=False)\n",
    "pred_df.head()"
   ],
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-21T04:35:01.204358Z",
     "start_time": "2024-10-21T04:34:50.189496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      session_type                                             labels\n",
       "0  12899779_clicks  59625 1253524 737445 438191 731692 1790770 942...\n",
       "1  12899780_clicks  1142000 736515 973453 582732 1502122 889686 17...\n",
       "2  12899781_clicks  918667 199008 194067 57315 141736 1460571 7594...\n",
       "3  12899782_clicks  834354 595994 740494 889671 987399 779477 1344...\n",
       "4  12899783_clicks  1817895 607638 1754419 1216820 1729553 300127 ..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12899779_clicks</td>\n",
       "      <td>59625 1253524 737445 438191 731692 1790770 942...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12899780_clicks</td>\n",
       "      <td>1142000 736515 973453 582732 1502122 889686 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12899781_clicks</td>\n",
       "      <td>918667 199008 194067 57315 141736 1460571 7594...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12899782_clicks</td>\n",
       "      <td>834354 595994 740494 889671 987399 779477 1344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12899783_clicks</td>\n",
       "      <td>1817895 607638 1754419 1216820 1729553 300127 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 173
  }
 ]
}
